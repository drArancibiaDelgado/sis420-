{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Box' object has no attribute 'n'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 63\u001b[0m\n\u001b[0;32m     60\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 63\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m15000\u001b[39;49m\u001b[43m)\u001b[49m        \n",
      "Cell \u001b[1;32mIn[2], line 6\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(episodes)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(episodes):\n\u001b[0;32m      4\u001b[0m     env \u001b[38;5;241m=\u001b[39m gym\u001b[38;5;241m.\u001b[39mmake(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCarRacing-v2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m     q_table \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros([\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobservation_space\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn\u001b[49m, env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mn])\n\u001b[0;32m      8\u001b[0m     learning_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.1\u001b[39m\n\u001b[0;32m      9\u001b[0m     discount_factor \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.95\u001b[39m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Box' object has no attribute 'n'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def train(episodes):\n",
    "    env = gym.make(\"CliffWalking-v0\")\n",
    "    \n",
    "    q_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "    \n",
    "    learning_rate = 0.1\n",
    "    discount_factor = 0.95\n",
    "    epsilon = 1  # EXPLORACION 1\n",
    "    epsilon_decay_rate = 0.0001  # EXPLORACION 2\n",
    "    rng = np.random.default_rng()  # genera numeros aleatorios\n",
    "    \n",
    "    rewards_per_episode = np.zeros(episodes)\n",
    "    \n",
    "    # bucle\n",
    "    for i in range(episodes):\n",
    "        if (i+1) % 1000 == 0:\n",
    "            env.close()\n",
    "            env = gym.make(\"CliffWalking-v0\", render_mode=\"human\")\n",
    "        else:\n",
    "            env.close()\n",
    "            env = gym.make(\"CliffWalking-v0\")\n",
    "            \n",
    "        state = env.reset()[0]\n",
    "        \n",
    "        terminated = False\n",
    "        truncated = False\n",
    "        \n",
    "        while (not terminated and not truncated):\n",
    "            \n",
    "            if rng.random() < epsilon:\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                action = np.argmax(q_table[state, :])\n",
    "                \n",
    "            new_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            \n",
    "            q_table[state, action] = q_table[state, action] + learning_rate * (reward + discount_factor * np.max(q_table[new_state, :]) - q_table[state, action])\n",
    "            \n",
    "            state = new_state\n",
    "            \n",
    "        epsilon = max(epsilon - epsilon_decay_rate, 0)\n",
    "    \n",
    "        if reward == 1:\n",
    "            rewards_per_episode[i] = 1\n",
    "            \n",
    "        if (i+1) % 100 == 0:\n",
    "            print(f\"Episode: {i+1} - reward: {rewards_per_episode[i]}\") \n",
    "        \n",
    "    env.close()\n",
    "        \n",
    "    print(f\"Mejor Q: {q_table}\")\n",
    "        \n",
    "    sum_rewards = np.zeros(episodes)\n",
    "    for t in range(episodes):\n",
    "        sum_rewards[t] = np.sum(rewards_per_episode[max(0, t-100):(t+1)])\n",
    "        \n",
    "    plt.plot(sum_rewards)\n",
    "    plt.show()\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    train(15000)        \n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
