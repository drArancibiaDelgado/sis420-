{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aprendisaje por refuerzo rompecabezas\n",
    "Diego Roberto Arancbia Delgado\n",
    "link del repositorio:https://github.com/drArancibiaDelgado/sis420-/tree/main/Examenes/Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Implementación del entorno del rompecabezas \n",
    "class PuzzleEnv:\n",
    "    def __init__(self):\n",
    "        self.n_rows = 4\n",
    "        self.n_cols = 5\n",
    "        self.state = np.arange(self.n_rows * self.n_cols).reshape((self.n_rows, self.n_cols))\n",
    "        np.random.shuffle(self.state)\n",
    "        self.goal_state = np.arange(self.n_rows * self.n_cols).reshape((self.n_rows, self.n_cols))\n",
    "\n",
    "    def reset(self):\n",
    "        np.random.shuffle(self.state)\n",
    "        return self._get_state()\n",
    "\n",
    "    def _get_state(self):\n",
    "        return tuple(map(tuple, self.state))  # Convertimos la matriz en una tupla de tuplas para que sea hashable\n",
    "\n",
    "    def step(self, action):\n",
    "        # Encontrar la posición de la celda vacía (representada por el valor 0)\n",
    "        zero_pos = np.argwhere(self.state == 0)[0]\n",
    "        row, col = zero_pos\n",
    "\n",
    "        # Determinar la nueva posición del vacío basado en la acción\n",
    "        if action == 0 and row > 0:  # Arriba\n",
    "            new_row, new_col = row - 1, col\n",
    "        elif action == 1 and row < self.n_rows - 1:  # Abajo\n",
    "            new_row, new_col = row + 1, col\n",
    "        elif action == 2 and col > 0:  # Izquierda\n",
    "            new_row, new_col = row, col - 1\n",
    "        elif action == 3 and col < self.n_cols - 1:  # Derecha\n",
    "            new_row, new_col = row, col + 1\n",
    "        else:\n",
    "            return self._get_state(), -1, False  # Acción inválida\n",
    "\n",
    "        # Intercambiar los valores de la celda vacía y la celda objetivo\n",
    "        self.state[row, col], self.state[new_row, new_col] = self.state[new_row, new_col], self.state[row, col]\n",
    "\n",
    "        # Calcular la recompensa\n",
    "        reward = 1 if np.array_equal(self.state, self.goal_state) else -0.1\n",
    "\n",
    "        # Comprobar si el rompecabezas está resuelto\n",
    "        done = np.array_equal(self.state, self.goal_state)\n",
    "\n",
    "        return self._get_state(), reward, done\n",
    "\n",
    "    def render(self):\n",
    "        print(self.state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "#la formula que se utiliza para el qlearning en este codigo es la siguiente\n",
    "#Q(s,a) = Q(s,a) + alpha * (reward + gamma * max(Q(s',a')) - Q(s,a))\n",
    "# se define una clase QLearningAgent que tiene los siguientes metodos\n",
    "# __init__ = inicializa los valores de alpha, gamma, epsilon y la tabla q\n",
    "# choose_action = elige una accion aleatoria o la mejor accion segun la tabla q\n",
    "# update = actualiza la tabla q segun la formula de qlearning\n",
    "  \n",
    "class QLearningAgent:\n",
    "    def __init__(self, n_actions, state_shape, alpha=0.5, gamma=0.99, epsilon=0.8):\n",
    "        self.n_actions = n_actions\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.q_table = {}\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        if random.uniform(0, 1) < self.epsilon:\n",
    "            return random.randint(0, self.n_actions - 1)\n",
    "        else:\n",
    "            if state not in self.q_table:\n",
    "                self.q_table[state] = np.zeros(self.n_actions)\n",
    "            return np.argmax(self.q_table[state])\n",
    "\n",
    "    def update(self, state, action, reward, next_state):\n",
    "        #glosario\n",
    "        #state = estado actual\n",
    "        #action = accion actual\n",
    "        #reward = recompensa\n",
    "        #next_state = estado siguiente\n",
    "        #Q(s,a) = Q(s,a) + alpha * (reward + gamma * max(Q(s',a')) - Q(s,a))\n",
    "        if state not in self.q_table:\n",
    "            self.q_table[state] = np.zeros(self.n_actions)\n",
    "        if next_state not in self.q_table:\n",
    "            self.q_table[next_state] = np.zeros(self.n_actions)\n",
    "        best_next_action = np.argmax(self.q_table[next_state])\n",
    "        td_target = reward + self.gamma * self.q_table[next_state][best_next_action]\n",
    "        td_error = td_target - self.q_table[state][action]\n",
    "        self.q_table[state][action] += self.alpha * td_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 completed\n",
      "Episode 5000 completed\n",
      "Episode 10000 completed\n",
      "Episode 15000 completed\n",
      "Episode 20000 completed\n",
      "Episode 25000 completed\n",
      "Episode 30000 completed\n",
      "Episode 35000 completed\n",
      "Episode 40000 completed\n",
      "Episode 45000 completed\n",
      "Episode 50000 completed\n",
      "Episode 55000 completed\n",
      "Episode 60000 completed\n",
      "Episode 65000 completed\n",
      "Episode 70000 completed\n",
      "Episode 75000 completed\n",
      "Episode 80000 completed\n",
      "Episode 85000 completed\n",
      "Episode 90000 completed\n",
      "Episode 95000 completed\n",
      "Training finished\n",
      "[[15 10  2  5  0]\n",
      " [12 19  8 18  9]\n",
      " [16 11  3 17 14]\n",
      " [13  7  1  4  6]]\n"
     ]
    }
   ],
   "source": [
    "# Se crea una instancia del entorno y del agente, y se entrena el agente\n",
    "\n",
    "env = PuzzleEnv()\n",
    "agent = QLearningAgent(n_actions=4, state_shape=(env.n_rows, env.n_cols))\n",
    "\n",
    "n_episodes = 100000\n",
    "max_steps_per_episode = 100\n",
    "\n",
    "for episode in range(n_episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    steps = 0\n",
    "\n",
    "    while not done and steps < max_steps_per_episode:\n",
    "        action = agent.choose_action(state)\n",
    "        next_state, reward, done = env.step(action)\n",
    "        agent.update(state, action, reward, next_state)\n",
    "        state = next_state\n",
    "        steps += 1\n",
    "\n",
    "    if episode % 5000 == 0:\n",
    "        print(f\"Episode {episode} completed\")\n",
    "\n",
    "print(\"Training finished\")\n",
    "env.render()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
