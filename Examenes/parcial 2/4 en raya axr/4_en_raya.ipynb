{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMtH0Xsrgclm"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/juansensio/axr/blob/master/axr/00_intro.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h1> APRENDIZAJE POR REFUERZO</h1>\n",
        "<h2> ING DE SISTEMAS</h2>\n",
        "<h3> Diego Roberto Arancibia Delgado</h3>\n",
        "<h4>link del repositorio:https://github.com/drArancibiaDelgado/sis420-/tree/main/Examenes/parcial%202<h4>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "48FoiBrmgclz"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class Board():\n",
        "    #aqui es donde se define el tablero y se inicializa, ademas de su tamaño\n",
        "    def __init__(self):\n",
        "        self.state = np.zeros((4,4))\n",
        "\n",
        "    def valid_moves(self):\n",
        "        return [(i, j) for j in range(4) for i in range(4) if self.state[i, j] == 0]\n",
        "\n",
        "    def update(self, symbol, row, col):\n",
        "        if self.state[row, col] == 0:\n",
        "            self.state[row, col] = symbol\n",
        "        else:\n",
        "            raise ValueError (\"movimiento ilegal !\")\n",
        "\n",
        "    def is_game_over(self):\n",
        "        # comprobar filas y columnas\n",
        "        if (self.state.sum(axis=0) == 4).sum() >= 1 or (self.state.sum(axis=1) == 4).sum() >= 1:\n",
        "            return 1\n",
        "        if (self.state.sum(axis=0) == -4).sum() >= 1 or (self.state.sum(axis=1) == -4).sum() >= 1:\n",
        "            return -1\n",
        "        # comprobar diagonales\n",
        "        diag_sums = [\n",
        "            sum([self.state[i, i] for i in range(4)]),\n",
        "            sum([self.state[i, 4 - i - 1] for i in range(4)]),\n",
        "        ]\n",
        "        if diag_sums[0] == 4 or diag_sums[1] == 4:\n",
        "            return 1\n",
        "        if diag_sums[0] == -4 or diag_sums[1] == -4:\n",
        "            return -1\n",
        "        # empate\n",
        "        if len(self.valid_moves()) == 0:\n",
        "            return 0\n",
        "        # seguir jugando\n",
        "        return None\n",
        "\n",
        "    def reset(self):\n",
        "        self.state = np.zeros((4,4))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "JHrTDE67gcl0"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "class Game():\n",
        "    def __init__(self, player1, player2):\n",
        "        player1.symbol = 1\n",
        "        player2.symbol = -1\n",
        "        self.players = [player1, player2]\n",
        "        self.board = Board()\n",
        "\n",
        "    def selfplay(self, rounds=100):\n",
        "        wins = [0, 0]\n",
        "        for i in tqdm(range(1, rounds + 1)):\n",
        "            self.board.reset()\n",
        "            for player in self.players:\n",
        "                player.reset()\n",
        "            game_over = False\n",
        "            while not game_over:\n",
        "                for player in self.players:\n",
        "                    action = player.move(self.board)\n",
        "                    self.board.update(player.symbol, action[0], action[1])\n",
        "                    for player in self.players:\n",
        "                        player.update(self.board)\n",
        "                    if self.board.is_game_over() is not None:\n",
        "                        game_over = True\n",
        "                        break\n",
        "            self.reward()\n",
        "            for ix, player in enumerate(self.players):\n",
        "                if self.board.is_game_over() == player.symbol:\n",
        "                    wins[ix] += 1\n",
        "        return wins\n",
        "\n",
        "\n",
        "    def reward(self):\n",
        "        winner = self.board.is_game_over()\n",
        "        if winner == 0: # empate\n",
        "            for player in self.players:\n",
        "                player.reward(0.5)\n",
        "        else: # le damos 1 recompensa al jugador que gana\n",
        "            for player in self.players:\n",
        "                if winner == player.symbol:\n",
        "                    player.reward(1)\n",
        "                else:\n",
        "                    player.reward(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "vfaI-brLgcl1"
      },
      "outputs": [],
      "source": [
        "class Agent():\n",
        "    def __init__(self, alpha=0.5, prob_exp=0.5):\n",
        "        self.value_function = {} # tabla con pares estado -> valor\n",
        "        self.alpha = alpha         # learning rate\n",
        "        self.positions = []       # guardamos todas las posiciones de la partida\n",
        "        self.prob_exp = prob_exp   # probabilidad de explorar\n",
        "\n",
        "    def reset(self):\n",
        "        self.positions = []\n",
        "\n",
        "    def move(self, board, explore=True):\n",
        "        valid_moves = board.valid_moves()\n",
        "        # exploracion\n",
        "        if explore and np.random.uniform(0, 1) < self.prob_exp:\n",
        "            # vamos a una posición aleatoria\n",
        "            ix = np.random.choice(len(valid_moves))\n",
        "            return valid_moves[ix]\n",
        "        # explotacion\n",
        "        # vamos a la posición con más valor\n",
        "        max_value = -1000\n",
        "        for row, col in valid_moves:\n",
        "            next_board = board.state.copy()\n",
        "            next_board[row, col] = self.symbol\n",
        "            next_state = str(next_board.reshape(4*4))\n",
        "            value = 0 if self.value_function.get(next_state) is None else self.value_function.get(next_state)\n",
        "            if value >= max_value:\n",
        "                max_value = value\n",
        "                best_row, best_col = row, col\n",
        "        return best_row, best_col\n",
        "\n",
        "    def update(self, board):\n",
        "        self.positions.append(str(board.state.reshape(4*4)))\n",
        "\n",
        "\n",
        "    def reward(self, reward):\n",
        "        # al final de la partida (cuando recibimos la recompensa)\n",
        "        # iteramos por tods los estados actualizando su valor en la tabla\n",
        "        for p in reversed(self.positions):\n",
        "            if self.value_function.get(p) is None:\n",
        "                self.value_function[p] = 0\n",
        "            self.value_function[p] += self.alpha * (reward - self.value_function[p])\n",
        "            reward = self.value_function[p]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M_mG5T7egcl1",
        "outputId": "6b208005-ec76-48eb-84dc-2ee8ef493edd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 300000/300000 [39:29<00:00, 126.63it/s]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[107100, 82872]"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "agent1 = Agent(prob_exp=0.5)\n",
        "agent2 = Agent()\n",
        "\n",
        "game = Game(agent1, agent2)\n",
        "\n",
        "game.selfplay(300000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "iMqFyZ2Vgcl2",
        "outputId": "9f84608d-18de-452f-fa3d-1fa6091e6d8d"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>estado</th>\n",
              "      <th>valor</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[ 0. -1.  0.  1.  0.  0. -1.  1.  0.  0.  0.  ...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[ 0. -1.  0.  1.  0.  0. -1.  1.  0.  0.  0.  ...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[-1. -1.  0.  1.  0.  0. -1.  1.  0.  0.  0.  ...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[ 1.  0.  0. -1.  1.  0.  0.  0.  1.  0.  0. -...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[ 0. -1. -1.  1.  0.  0.  0.  1.  0.  0. -1.  ...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1120502</th>\n",
              "      <td>[ 0.  0.  0.  1. -1.  1. -1.  0. -1.  1.  0.  ...</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1120503</th>\n",
              "      <td>[-1. -1. -1.  1.  0. -1.  1.  1.  0.  0. -1.  ...</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1120504</th>\n",
              "      <td>[-1.  1.  0.  1. -1. -1. -1. -1.  0.  0.  1.  ...</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1120505</th>\n",
              "      <td>[ 1.  0.  1.  1. -1. -1. -1. -1.  0.  1.  0.  ...</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1120506</th>\n",
              "      <td>[ 1.  0.  1.  1.  0. -1. -1. -1.  0.  1.  0.  ...</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1120507 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                    estado  valor\n",
              "0        [ 0. -1.  0.  1.  0.  0. -1.  1.  0.  0.  0.  ...    1.0\n",
              "1        [ 0. -1.  0.  1.  0.  0. -1.  1.  0.  0.  0.  ...    1.0\n",
              "2        [-1. -1.  0.  1.  0.  0. -1.  1.  0.  0.  0.  ...    1.0\n",
              "3        [ 1.  0.  0. -1.  1.  0.  0.  0.  1.  0.  0. -...    1.0\n",
              "4        [ 0. -1. -1.  1.  0.  0.  0.  1.  0.  0. -1.  ...    1.0\n",
              "...                                                    ...    ...\n",
              "1120502  [ 0.  0.  0.  1. -1.  1. -1.  0. -1.  1.  0.  ...    0.0\n",
              "1120503  [-1. -1. -1.  1.  0. -1.  1.  1.  0.  0. -1.  ...    0.0\n",
              "1120504  [-1.  1.  0.  1. -1. -1. -1. -1.  0.  0.  1.  ...    0.0\n",
              "1120505  [ 1.  0.  1.  1. -1. -1. -1. -1.  0.  1.  0.  ...    0.0\n",
              "1120506  [ 1.  0.  1.  1.  0. -1. -1. -1.  0.  1.  0.  ...    0.0\n",
              "\n",
              "[1120507 rows x 2 columns]"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "funcion_de_valor = sorted(agent1.value_function.items(), key=lambda kv: kv[1], reverse=True)\n",
        "tabla = pd.DataFrame({'estado': [x[0] for x in funcion_de_valor], 'valor': [x[1] for x in funcion_de_valor]})\n",
        "\n",
        "tabla"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8eKvwrrxgcl2"
      },
      "source": [
        "Este ejemplo sirve para ilustrar algunas de las propiedades clave del axr. En primer lugar, aprender a través de la interacción con el entorno (en este caso el otro agente). En segundo lugar, tenemos un objetivo claro y el comportamiento correcto del agente requiere de planificación y predicción que tenga en cuenta los efectos futuros de sus acciones."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5901i3rPgcl3"
      },
      "source": [
        "##  Resumen"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqUALSkHgcl3"
      },
      "source": [
        "El aprendizaje por refuerzo es una aproximación computacional a la comprensión y automatización del aprendizaje por objetivos y toma de decisiones. En esta aproximación, una agente aprende a través de la interacción directa con su entorno sin necesidad de supervisión explícita. Utiliza procesos de decisión de Markov para definir la interacción entre el agente y su entorno en términos de estados, acciones y recompensas. Los conceptos de valor y función de valor son la clave de muchos métodos de axr ya que representan una manera eficiente de búsqueda en el espacio de políticas."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "interpreter": {
      "hash": "bb9f406c0f70fca9801e60f2cbb7cd1ccff2ae2f74c58f513340bcf6cae5ecd0"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
